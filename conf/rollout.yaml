# conf/rollout.yaml (Qwen3-4B, full finetune)
actor_rollout_ref:
  model:
    partial_pretrain: Qwen/Qwen3-4B
  rollout:
    name: "sglang"
    multi_turn: true
    format: "chatml"
    tool_config_path: "tools/kernel_tool.yaml"
    temperature: 0.7
    top_p: 0.9
    n: 1
  ref:
    model:
      partial_pretrain: Qwen/Qwen3-4B

data:
  train_files: "data/kernelbench_train.parquet"
  val_files:   "data/kernelbench_holdout.parquet"
  prompt_key:  "prompt"
  train_batch_size: 16
  max_prompt_length: 1024
  max_response_length: 1024

# —— GRPO‑specific knobs ——
algorithm:
  adv_estimator: grpo
  use_kl_in_reward: false
  kl_penalty: kl
  kl_ctrl:
    type: fixed
    kl_coef: 0.02
    target_kl: 0.1
  actor_rollout_ref:
    actor:
      loss_agg_mode: token-mean

reward_model:
  module_path: "kevin_reward.compute_score"

trainer:
  project_name: "kevin-grpo"
  experiment_name: "kevin-multiturn-grpo-qwen3-4b"
  total_epochs: 4
  micro_batch_size_per_gpu: 4
  global_batch_size: 64
  default_hdfs_dir: "hdfs://user/verl/experiments/kevin_grpo/"
  logger: ["console", "wandb"]
