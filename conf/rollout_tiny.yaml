# conf/rollout_tiny_lora.yaml (Qwen2.5-0.5B-Instruct, LoRA)
actor_rollout_ref:
  model:
    partial_pretrain: Qwen/Qwen2.5-0.5B-Instruct
    # ---- LoRA knobs ----
    lora_rank: 16            # try 8 if you still OOM
    lora_alpha: 16
    target_modules: "all-linear"
    use_shm: true            # faster reloads, safe on single‑GPU
  rollout:
    name: "vllm"             # LoRA needs vLLM backend
    load_format: "safetensors"
    layered_summon: true     # save peak mem when syncing adapters
    multi_turn: false        # vLLM is single‑turn for now
    format: "chatml"
    tool_config_path: "tools/kernel_tool.yaml"
    temperature: 0.7
    top_p: 0.9
    n: 1
  ref:
    model:
      partial_pretrain: Qwen/Qwen2.5-0.5B-Instruct

data:
  train_files: "data/kernelbench_train.parquet"
  val_files:   "data/kernelbench_holdout.parquet"
  prompt_key:  "prompt"
  train_batch_size: 16
  max_prompt_length: 1024
  max_response_length: 1024

# ---- GRPO knobs (unchanged) ----
algorithm:
  adv_estimator: grpo
  use_kl_in_reward: false
  kl_penalty: kl
  kl_ctrl:
    type: fixed
    kl_coef: 0.02
    target_kl: 0.1
  actor_rollout_ref:
    actor:
      loss_agg_mode: token-mean

reward_model:
  module_path: "kevin_reward.compute_score"

trainer:
  project_name: "kevin-grpo"
  experiment_name: "kevin-grpo-qwen0.5b-lora"
  total_epochs: 4
  micro_batch_size_per_gpu: 1      # keep it tiny
  global_batch_size: 16            # accumulate grad to reach this
  default_hdfs_dir: "hdfs://user/verl/experiments/kevin_grpo_lora/"
  logger: ["console", "wandb"]

# ---- memory savers ----
actor_rollout_ref:
  actor:
    fsdp_config:
      param_offload: true
      optimizer_offload: true
